/*
 * -------------------------------------------------
 *  Nextflow config file for running Polish my Reads! on MfN cluster
 * -------------------------------------------------
 *   Use as follows:
 *   nextflow run main.nf -profile mfn
 */

executor {
    name = 'slurm'
    // The number of tasks the executor will handle in a parallel manner (default: 100).
    queueSize = 200
    // Determines the max rate of job submission per time unit, for example '10sec' eg. max 10 jobs per second
    submitRateLimit = '50sec'
}

process {
    config_profile_name = 'MfN cluster profile'
    config_profile_description = 'Profile that works well with the Museum fur naturkunde cluster'
    executor = 'slurm'
    queue = 'Test'
    cpus = 1
    memory = '2 GB'
    time = '2h'
    conda = '/home/mobl/anaconda3/envs/nf-phylo_test'

    errorStrategy = 'retry'
    maxRetries = 3

    // This pipeline can be computationally intensive and needs to be adjusted by genome and sampling.
    // The settings below work for a 50 individual avian genomics project.
    // Please note that if you change the label, the nextflow script itself will also need to be adjusted.
    // Each computationally intensive process has been tagged with a guesstimate of requirements. I've used
    // a labeling scheme so they can be adjusted based on genome/dataset requirements.
    // The labeling code below refers to: H(igh)/I(ntermediate)/S(mall), C(ores)/M(em)/T(ime). In example:
    // HC_IM_ST = High number of cores, Intermediate amount of RAM and small amount of time requested.

    withLabel: 'SC_SM_IT' {
        cpus = 1
        memory = { 2.GB * task.attempt }
        time = { 12.h * task.attempt }
    }

    withLabel: 'SC_SM_HT' {
        cpus = 1
        memory = { 2.GB * task.attempt }
        time = { 24.h * task.attempt }
    }

    withLabel: 'ete3' {
        conda = '/home/mobl/anaconda3/envs/ete3'
        memory = { 4.GB * task.attempt }
    }

}
