/*
 * -------------------------------------------------
 *  Nextflow config file for running Polish my Reads! on MfN cluster
 * -------------------------------------------------
 *   Use as follows:
 *   nextflow run main.nf -profile mfn
 */

executor {
    name = 'slurm'
    // The number of tasks the executor will handle in a parallel manner (default: 100).
    queueSize = 200
    // Determines the max rate of job submission per time unit, for example '10sec' eg. max 10 jobs per second
    submitRateLimit = '50sec'
}

process {

    config_profile_name = 'Curta cluster profile'
    config_profile_description = 'Profile that works well with the Freie Unvirsity Curta cluster'
    executor = 'slurm'
    queue = 'begendiv'
    clusterOptions = '--qos=prio'
    cpus = 1
    memory = '2 GB'
    time = '2h'
    beforeScript = 'module load Anaconda3'
    conda = '/home/mobl/.conda/envs/nf_phylo'

    // This pipeline can be computationally intensive and needs to be adjusted by genome and sampling.
    // The settings below work for a 50 individual avian genomics project.
    // Please note that if you change the label, the nextflow script itself will also need to be adjusted.
    // Each computationally intensive process has been tagged with a guesstimate of requirements. I've used
    // a labeling scheme so they can be adjusted based on genome/dataset requirements.
    // The labeling code below refers to: H(igh)/I(ntermediate)/S(mall), C(ores)/M(em)/T(ime). In example:
    // HC_IM_ST = High number of cores, Intermediate amount of RAM and small amount of time requested.

    withLabel: 'SC_SM_IT' {
        cpus = 1
        memory = '4 GB'
        clusterOptions = '--qos=standard'
        time = '12h'
    }

    withLabel: 'SC_SM_HT' {
        cpus = 1
        memory = '4 GB'
        clusterOptions = '--qos=standard'
        time = '24h'
    }

    withLabel: 'ete3' {
	conda = '/home/mobl/.conda/envs/ete3'
    }

}
